# GPT-From-Scratch
A cutting-edge Generative Pretrained Transformer (GPT) project from scratch, inspired by the groundbreaking research outlined in the widely acclaimed paper "Attention is All You Need" and the latest advancements in the field, including OpenAI's GPT-2 and GPT-3, to create a highly effective and efficient transformer model.

## AIM
The aim of this project is to create a highly effective and efficient transformer model inspired by nanoGPT by Andrej Karpathy, which is a 2.7B parameter transformer model that can be trained on a single GPU. The model will be trained on the [Project Gutenberg](https://www.gutenberg.org/) dataset, which contains over 60,000 free eBooks. The model will be trained to generate text, and will be evaluated on the basis of its ability to generate text that is coherent and relevant to the prompt.

## PROGRESS
- [x] Prepare required data for training and validation of the model
